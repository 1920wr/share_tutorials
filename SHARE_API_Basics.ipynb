{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling the SHARE API\n",
    "----\n",
    "Here are some working examples of how to query the current scrAPI database for metrics of results coming through the SHARE Notifiation Service.\n",
    "\n",
    "These particular queries are just examples, and the data is open for anyone to use, so feel free to make your own and experiment!\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "It's good practice in python to import all the modules you're going to use up at the top, so we'll go ahead and do that now. We might not use all of these imports until later, but we'll get everything over with at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import json\n",
    "import furl\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "\n",
    "from sharepa import ShareSearch\n",
    "from sharepa import basic_search\n",
    "from sharepa import merge_dataframes\n",
    "from sharepa import bucket_to_dataframe\n",
    "from sharepa.helpers import pretty_print\n",
    "from sharepa.helpers import source_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Service Names for Reference\n",
    "----\n",
    "Each provider harvested from uses a shortened name for its source. Let's make an API call to generate a table to get all of those short names, along with the official name of the repository that they represent.\n",
    "\n",
    "The SHARE API has different endpoints. One of those endpoints returns a list of all of the providers that SHARE is harvesting from, along with their short names, official names, links to their homepages, and a simple version of an icon representing their service, in a parsable format called json.\n",
    "\n",
    "Let's make a call to that API endpoint using the requests libarary, get the json data, and print out all of the shortnames and longnames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doepages - Department of Energy Pages\n",
      "scholarsarchiveosu - ScholarsArchive@OSU\n",
      "utaustin - University of Texas at Austin Digital Repository\n",
      "scholarworks_umass - ScholarWorks@UMass Amherst\n",
      "cambridge - Apollo @ University of Cambridge\n",
      "texasstate - DSpace at Texas State University\n",
      "osf - Open Science Framework\n",
      "lwbin - Lake Winnipeg Basin Information Network\n",
      "uow - Research Online @ University of Wollongong\n",
      "oaktrust - The OAKTrust Digital Repository at Texas A&M\n",
      "umich - Deep Blue @ University of Michigan\n",
      "cogprints - Cognitive Sciences ePrint Archive\n",
      "utktrace - Trace: Tennessee Research and Creative Exchange\n",
      "stcloud - The repository at St Cloud State\n",
      "smithsonian - Smithsonian Digital Repository\n",
      "csuohio - Cleveland State University's EngagedScholarship@CSU\n",
      "biomedcentral - BioMed Central\n",
      "crossref - CrossRef\n",
      "purdue - PURR - Purdue University Research Repository\n",
      "unl_digitalcommons - DigitalCommons@University of Nebraska - Lincoln\n",
      "scholarscompass_vcu - VCU Scholars Compass\n",
      "dataone - DataONE: Data Observation Network for Earth\n",
      "cmu - Carnegie Mellon University Research Showcase\n",
      "nist - NIST MaterialsData\n",
      "udel - University of Delaware Institutional Repository\n",
      "rcaap - RCAAP - Repositório Científico de Acesso Aberto de Portugal\n",
      "calhoun - Calhoun: Institutional Archive of the Naval Postgraduate School\n",
      "mason - Mason Archival Repository Service\n",
      "kent - Digital Commons @ Kent State University Libraries\n",
      "asu - Arizona State University Digital Repository\n",
      "zenodo - Zenodo\n",
      "calpoly - Digital Commons @ CalPoly\n",
      "ghent - Ghent University Academic Bibliography\n",
      "neurovault - NeuroVault.org\n",
      "plos - Public Library of Science\n",
      "tdar - The Digital Archaeological Record\n",
      "vtech - Virginia Tech VTechWorks\n",
      "ncar - Earth System Grid at NCAR\n",
      "duke - Duke University Libraries\n",
      "scholarsbank - Scholars Bank University of Oregon\n",
      "datacite - DataCite MDS\n",
      "huskiecommons - Huskie Commons @ Northern Illinois University\n",
      "uky - UKnowledge @ University of Kentucky\n",
      "trinity - Digital Commons @ Trinity University\n",
      "pdxscholar - PDXScholar Portland State University\n",
      "iastate - Digital Repository @ Iowa State University\n",
      "sldr - Speech and Language Data Repository (SLDR/ORTOLANG)\n",
      "cuscholar - CU Scholar University of Colorado Boulder\n",
      "pubmedcentral - PubMed Central\n",
      "cyberleninka - CyberLeninka - Russian open access scientific library\n",
      "mblwhoilibrary - WHOAS at MBLWHOI Library\n",
      "springer - Springer\n",
      "shareok - SHAREOK Repository\n",
      "dryad - Dryad Data Repository\n",
      "spdataverse - Scholars Portal dataverse\n",
      "noaa_nodc - National Oceanographic Data Center\n",
      "bhl - Biodiversity Heritage Library OAI Repository\n",
      "pcurio - Pontifical Catholic University of Rio de Janeiro\n",
      "dash - Digital Access to Scholarship at Harvard\n",
      "lshtm - London School of Hygiene and Tropical Medicine Research Online\n",
      "waynestate - Digital Commons @ Wayne State\n",
      "opensiuc - OpenSIUC at the Southern Illinois University Carbondale\n",
      "hacettepe - Hacettepe University DSpace on LibLiveCD\n",
      "ucescholarship - eScholarship @ University of California\n",
      "icpsr - Inter-University Consortium for Political and Social Research\n",
      "dailyssrn - Social Science Research Network\n",
      "caltech - CaltechAUTHORS\n",
      "wash_state_u - Washington State University Research Exchange\n",
      "harvarddataverse - Harvard Dataverse\n",
      "scitech - DoE's SciTech Connect Database\n",
      "mit - DSpace@MIT\n",
      "erudit - Érudit\n",
      "nih - NIH Research Portal Online Reporting Tools\n",
      "pcom - DigitalCommons@PCOM\n",
      "columbia - Columbia Academic Commons\n",
      "figshare - figshare\n",
      "citeseerx - CiteSeerX Scientific Literature Digital Library and Search Engine\n",
      "addis_ababa - Addis Ababa University Institutional Repository\n",
      "arxiv_oai - ArXiv\n",
      "digitalhoward - Digital Howard @ Howard University\n",
      "wustlopenscholarship - Washington University Open Scholarship\n",
      "clinicaltrials - ClinicalTrials.gov\n",
      "iowaresearch - Iowa Research Online\n",
      "valposcholar - Valparaiso University ValpoScholar\n",
      "chapman - Chapman University Digital Commons\n",
      "upennsylvania - University of Pennsylvania Scholarly Commons\n",
      "uwashington - ResearchWorks @ University of Washington\n",
      "uiucideals - University of Illinois at Urbana-Champaign, IDEALS\n"
     ]
    }
   ],
   "source": [
    "data = requests.get('https://osf.io/api/v1/share/providers/').json()\n",
    "\n",
    "for source in data['providerMap'].keys():\n",
    "    print(\n",
    "        '{} - {}'.format(\n",
    "            data['providerMap'][source]['short_name'],\n",
    "            data['providerMap'][source]['long_name'].encode('utf-8')\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHARE Schema\n",
    "\n",
    "You can make queries against any of the fields defined in the [SHARE Schema](https://github.com/CenterForOpenScience/SHARE-Schema/blob/master/share.yaml). If we were able to harvest the information from the original source, it should appear in SHARE. However, not all fields are required for every document.\n",
    "\n",
    "Required fields include:\n",
    "- title\n",
    "- contributors\n",
    "- uris\n",
    "- providerUpdatedDateTime\n",
    "\n",
    "We add some information after each document is harvested inside the field shareProperties, including:\n",
    "- source (where the document was originally harvested)\n",
    "- docID  (a unique identifier for that object from that source)\n",
    "\n",
    "These two fields can be combined to make a unique document identifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a URL to use to access the SHARE API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OSF_APP_URL = 'https://osf.io/api/v1/share/search/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the first 3 results from the most basic query - the first page of the most recently updated research release events in SHARE.\n",
    "\n",
    "We'll use the URL parsing library furl to keep track of all of our arguments to the URL, because we'll be modifying them as we go along. We'll print the URL as we go to take a look at it, so we know what we're requesting.\n",
    "\n",
    "We'll print out the result's title, original source, and when it was updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The request URL is https://osf.io/api/v1/share/search/?size=3&sort=providerUpdatedDateTime\n",
      "----------\n",
      "Data -- from osf -- updated at 2016-01-24T08:58:15.339000+00:00\n",
      "Materials -- from osf -- updated at 2016-01-24T08:58:14.962000+00:00\n",
      "Methods and Measures -- from osf -- updated at 2016-01-24T08:58:14.821000+00:00\n"
     ]
    }
   ],
   "source": [
    "search_url = furl.furl(OSF_APP_URL)\n",
    "search_url.args['size'] = 3\n",
    "search_url.args['sort'] = 'providerUpdatedDateTime'\n",
    "recent_results = requests.get(search_url.url).json()\n",
    "\n",
    "print('The request URL is {}'.format(search_url.url))\n",
    "print('----------')\n",
    "for result in recent_results['results']:\n",
    "    print(\n",
    "        '{} -- from {} -- updated at {}'.format(\n",
    "            result['title'].encode('utf-8'),\n",
    "            result['shareProperties']['source'],\n",
    "            result['providerUpdatedDateTime']\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's limit that query to only documents mentioning \"giraffes\" somewhere in the title, description, or in any of the metadata. We'd do that by adding a query search term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The request URL is https://osf.io/api/v1/share/search/?size=3&sort=providerUpdatedDateTime&q=giraffes\n",
      "---------\n",
      "Odd creature was ancient ancestor of today’s giraffes -- from crossref -- updated at 2015-11-24T00:00:00+00:00\n",
      "Naturalized seeing/colonial vision : interrogating the display of races in late nineteenth century France -- from datacite -- updated at 2015-11-11T23:35:35+00:00\n",
      "Integration of complex shapes and natural patterns -- from datacite -- updated at 2015-11-07T03:06:45+00:00\n"
     ]
    }
   ],
   "source": [
    "search_url.args['q'] = 'giraffes'\n",
    "recent_results = requests.get(search_url.url).json()\n",
    "\n",
    "print('The request URL is {}'.format(search_url.url))\n",
    "print('---------')\n",
    "for result in recent_results['results']:\n",
    "    print(\n",
    "        '{} -- from {} -- updated at {}'.format(\n",
    "            result['title'].encode('utf-8'),\n",
    "            result['shareProperties']['source'],\n",
    "            result['providerUpdatedDateTime']\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's search for documents from the source mit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The request URL is https://osf.io/api/v1/share/search/?size=3&sort=providerUpdatedDateTime&q=shareProperties.source:mit\n",
      "---------\n",
      "The LSND and MiniBooNE Oscillation Searches at High [Delta]m[superscript 2] -- from mit -- updated at 2016-01-22T19:30:41+00:00\n",
      "Microstructural view of burrowing with a bioinspired digging robot -- from mit -- updated at 2016-01-22T19:21:08+00:00\n",
      "Six High-Precision Transits of Ogle-Tr-113b -- from mit -- updated at 2016-01-22T19:15:43+00:00\n"
     ]
    }
   ],
   "source": [
    "search_url.args['q'] = 'shareProperties.source:mit'\n",
    "recent_results = requests.get(search_url.url).json()\n",
    "\n",
    "recent_results\n",
    "\n",
    "print('The request URL is {}'.format(search_url.url))\n",
    "print('---------')\n",
    "for result in recent_results['results']:\n",
    "    print(\n",
    "        '{} -- from {} -- updated at {}'.format(\n",
    "            result['title'].encode('utf-8'),\n",
    "            result['shareProperties']['source'],\n",
    "            result['providerUpdatedDateTime']\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine the two and find documents from MIT that mention giraffes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The request URL is https://osf.io/api/v1/share/search/?size=3&sort=providerUpdatedDateTime&q=shareProperties.source:mit+AND+giraffes\n",
      "---------\n",
      "Giraffes, religion and conflict : essays in behavioral decision making -- from mit -- updated at 2015-08-03T20:00:59\n"
     ]
    }
   ],
   "source": [
    "search_url.args['q'] = 'shareProperties.source:mit AND giraffes'\n",
    "recent_results = requests.get(search_url.url).json()\n",
    "\n",
    "recent_results\n",
    "\n",
    "print('The request URL is {}'.format(search_url.url))\n",
    "print('---------')\n",
    "for result in recent_results['results']:\n",
    "    print(\n",
    "        '{} -- from {} -- updated at {}'.format(\n",
    "            result['title'].encode('utf-8'),\n",
    "            result['shareProperties']['source'],\n",
    "            result['providerUpdatedDateTime']\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Queries\n",
    "The SHARE Search API runs on elasticsearch - meaning that it can accept complicated queries that give you a wide variety of information.\n",
    "\n",
    "Here are some examples of how to make more complex queries using the raw elasticsearch results. You can read a [lot more about elasticsearch queries here](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://osf.io/api/v1/share/search/'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_url.args = None  # reset the args so that we remove our old query arguments.\n",
    "search_url.url # Show the URL that we'll be requesting to make sure the args were cleared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Setup\n",
    "\n",
    "We can define a few functions that we can reuse to make querying simpler. Elasticsearch queries are passed through as json blobs specifying how to return the information you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_share(url, query):\n",
    "    # A helper function that will use the requests library,\n",
    "    # pass along the correct headers,\n",
    "    # and make the query we want\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = json.dumps(query)\n",
    "    return requests.post(url, headers=headers, data=data, verify=False).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Some Queries\n",
    "The SHARE schema has many spots for information, and many of the original sources do not provide this information. We can do a query to find out if a certain field exists or not within certain records. The SHARE API is set up to not display the field if it is empty.\n",
    "\n",
    "Let's query for the first 5 all documents that have a sponsorship field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sponsorship_query = {\n",
    "    \"size\": 5,\n",
    "    \"query\": {\n",
    "        \"filtered\": {\n",
    "            \"filter\": {\n",
    "                \"exists\": {\n",
    "                    \"field\": \"sponsorships\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Phase III, Randomized, Comparative, Open-label Study of Intravenous Iron Isomaltoside 1000 (Monofer®) Administered as Maintenance Therapy by Single or Repeated Bolus Injections in Comparison With Intravenous Iron Sucrose in Subjects With Stage 5 Chronic Kidney Disease on Dialysis Therapy (CKD-5D) -- from source clinicaltrials -- sponsored by Pharmacosmos A/S \n",
      "-------------------\n",
      "Phase IB Study of FOLFIRINOX Plus PF-04136309 in Patients With Borderline Resectable and Locally Advanced Pancreatic Adenocarcinoma -- from source clinicaltrials -- sponsored by Washington University School of Medicine National Cancer Institute (NCI)\n",
      "-------------------\n",
      "Discontinuation of Infliximab Therapy in Patients With Crohn's Disease During Sustained Complete Remission: A National Multi-center, Double Blinded, Randomized, Placebo Controlled Study -- from source clinicaltrials -- sponsored by Copenhagen University Hospital at Herlev \n",
      "-------------------\n",
      "Temperature Evaluation by MRI Thermometry During Cervical Cooling -- from source clinicaltrials -- sponsored by University of Vermont Cryothermic Systems. Inc.\n",
      "-------------------\n",
      "Combination of Lenalidomide and Ofatumumab in Patients With Previously Treated Chronic Lymphocytic Leukemia and Small Lymphocytic Lymphoma (CLL/SLL) -- from source clinicaltrials -- sponsored by M.D. Anderson Cancer Center GlaxoSmithKline\n",
      "-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erin/.virtualenvs/tuts/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py:768: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "results = query_share(search_url.url, sponsorship_query)\n",
    "\n",
    "for item in results['results']:\n",
    "#     print(item['sponsorships'])\n",
    "    print('{} -- from source {} -- sponsored by {}'.format(\n",
    "            item['title'].encode('utf-8'),\n",
    "            item['shareProperties']['source'].encode('utf-8'),\n",
    "            ' '.join(\n",
    "                [sponsor['sponsor']['sponsorName'] for sponsor in item['sponsorships']]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print('-------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how many results do not have tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags_query = {\n",
    "    \"query\": {\n",
    "        \"query_string\": {\n",
    "            \"analyze_wildcard\": True, \n",
    "            \"query\": \"NOT tags:*\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4150115 results out of 4264118, or 97.33%, do not have tags.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erin/.virtualenvs/tuts/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py:768: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "results_with_tags = query_share(search_url.url, tags_query)\n",
    "total_results = requests.get(search_url.url).json()['count']\n",
    "results_percent = (float(results_with_tags['count'])/total_results)*100\n",
    "\n",
    "print(\n",
    "    '{} results out of {}, or {}%, do not have tags.'.format(\n",
    "        results_with_tags['count'],\n",
    "        total_results,\n",
    "        format(results_percent, '.2f')\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Aggregations\n",
    "\n",
    "While searching for individual results is useful, sharepa also lets you make aggregation queries that give you results across the entirety of the SHARE dataset at once. This is useful if you're curious about the completeness of data sets.\n",
    "\n",
    "For example, we can find the number of documents per source that are missing titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_titles_aggregation = {\n",
    "    \"query\": {\n",
    "        \"query_string\": {\n",
    "            \"analyze_wildcard\": True, \n",
    "            \"query\": \"NOT title:*\"\n",
    "        }\n",
    "    }, \n",
    "    \"aggs\": {\n",
    "        \"sources\": {\n",
    "            \"terms\": {\n",
    "                \"field\": \"_type\", # A field where the SHARE source is stored                \n",
    "                \"min_doc_count\": 0, \n",
    "                \"size\": 0  # Will return all sources, regardless if there are results\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modify the args to make sure we return the raw elasticsearch results\n",
    "search_url.args['raw'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataone has 263947 documents without titles\n",
      "biomedcentral has 22891 documents without titles\n",
      "citeseerx has 9366 documents without titles\n",
      "crossref has 4950 documents without titles\n",
      "smithsonian has 101 documents without titles\n",
      "pubmedcentral has 55 documents without titles\n",
      "datacite has 54 documents without titles\n",
      "bhl has 7 documents without titles\n",
      "figshare has 5 documents without titles\n",
      "scitech has 3 documents without titles\n",
      "caltech has 2 documents without titles\n",
      "iowaresearch has 2 documents without titles\n",
      "rcaap has 2 documents without titles\n",
      "dash has 1 documents without titles\n",
      "duke has 1 documents without titles\n",
      "icpsr has 1 documents without titles\n",
      "lshtm has 1 documents without titles\n",
      "mit has 1 documents without titles\n",
      "shareok has 1 documents without titles\n",
      "ucescholarship has 1 documents without titles\n",
      "uiucideals has 1 documents without titles\n",
      "addis_ababa has 0 documents without titles\n",
      "arxiv_oai has 0 documents without titles\n",
      "asu has 0 documents without titles\n",
      "calhoun has 0 documents without titles\n",
      "calpoly has 0 documents without titles\n",
      "cambridge has 0 documents without titles\n",
      "chapman has 0 documents without titles\n",
      "clinicaltrials has 0 documents without titles\n",
      "cmu has 0 documents without titles\n",
      "cogprints has 0 documents without titles\n",
      "columbia has 0 documents without titles\n",
      "csuohio has 0 documents without titles\n",
      "cuscholar has 0 documents without titles\n",
      "cyberleninka has 0 documents without titles\n",
      "dailyssrn has 0 documents without titles\n",
      "digitalhoward has 0 documents without titles\n",
      "doepages has 0 documents without titles\n",
      "dryad has 0 documents without titles\n",
      "erudit has 0 documents without titles\n",
      "ghent has 0 documents without titles\n",
      "hacettepe has 0 documents without titles\n",
      "harvarddataverse has 0 documents without titles\n",
      "huskiecommons has 0 documents without titles\n",
      "iastate has 0 documents without titles\n",
      "kent has 0 documents without titles\n",
      "lwbin has 0 documents without titles\n",
      "mason has 0 documents without titles\n",
      "mblwhoilibrary has 0 documents without titles\n",
      "ncar has 0 documents without titles\n",
      "neurovault has 0 documents without titles\n",
      "nih has 0 documents without titles\n",
      "nist has 0 documents without titles\n",
      "noaa_nodc has 0 documents without titles\n",
      "oaktrust has 0 documents without titles\n",
      "opensiuc has 0 documents without titles\n",
      "osf has 0 documents without titles\n",
      "pcom has 0 documents without titles\n",
      "pcurio has 0 documents without titles\n",
      "pdxscholar has 0 documents without titles\n",
      "plos has 0 documents without titles\n",
      "purdue has 0 documents without titles\n",
      "scholarsarchiveosu has 0 documents without titles\n",
      "scholarsbank has 0 documents without titles\n",
      "scholarscompass_vcu has 0 documents without titles\n",
      "scholarworks_umass has 0 documents without titles\n",
      "sldr has 0 documents without titles\n",
      "spdataverse has 0 documents without titles\n",
      "springer has 0 documents without titles\n",
      "stcloud has 0 documents without titles\n",
      "tdar has 0 documents without titles\n",
      "texasstate has 0 documents without titles\n",
      "trinity has 0 documents without titles\n",
      "udel has 0 documents without titles\n",
      "uky has 0 documents without titles\n",
      "umich has 0 documents without titles\n",
      "unl_digitalcommons has 0 documents without titles\n",
      "uow has 0 documents without titles\n",
      "upennsylvania has 0 documents without titles\n",
      "utaustin has 0 documents without titles\n",
      "utktrace has 0 documents without titles\n",
      "uwashington has 0 documents without titles\n",
      "valposcholar has 0 documents without titles\n",
      "vtech has 0 documents without titles\n",
      "wash_state_u has 0 documents without titles\n",
      "waynestate has 0 documents without titles\n",
      "wustlopenscholarship has 0 documents without titles\n",
      "zenodo has 0 documents without titles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erin/.virtualenvs/tuts/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py:768: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "results_without_titles = query_share(search_url.url, missing_titles_aggregation)\n",
    "\n",
    "missing_titles_counts = results_without_titles['aggregations']['sources']['buckets']\n",
    "\n",
    "for source in missing_titles_counts:\n",
    "    print('{} has {} documents without titles'.format(source['key'], source['doc_count'], ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This information isn't terribly useful if we don't also know how many documents are in each source.\n",
    "\n",
    "Let's get that information as well, along stats for what percentage of documents from each source are missing titles. \n",
    "\n",
    "We'll do this with an elasticsearch \"sigificant terms\" aggregation. We're only interested in results that have 1 document or more, meaning all documents from the other sources have titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sig_terms_agg = {\n",
    "    \"query\": {\n",
    "        \"query_string\": {\n",
    "            \"analyze_wildcard\": True, \n",
    "            \"query\": \"NOT title:*\"\n",
    "        }\n",
    "    },\n",
    "    \"aggs\": {\n",
    "        \"sources\":{\n",
    "            \"significant_terms\":{\n",
    "                \"field\": \"_type\", # A field where the SHARE source is stored                \n",
    "                \"min_doc_count\": 1, # Only results with more than one document\n",
    "                \"percentage\": {} # This will make the \"score\" parameter a percentage\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erin/.virtualenvs/tuts/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py:768: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "docs_with_no_title_results = query_share(search_url.url, sig_terms_agg)\n",
    "docs_with_no_title = docs_with_no_title_results['aggregations']['sources']['buckets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biomedcentral has 22891/24831 or 92.19% with no titles\n",
      "dataone has 263947/356154 or 74.11% with no titles\n",
      "citeseerx has 9366/184037 or 5.09% with no titles\n",
      "smithsonian has 101/7327 or 1.38% with no titles\n",
      "crossref has 4950/911594 or 0.54% with no titles\n",
      "duke has 1/336 or 0.30% with no titles\n",
      "dash has 1/906 or 0.11% with no titles\n",
      "caltech has 2/5343 or 0.04% with no titles\n",
      "uiucideals has 1/3133 or 0.03% with no titles\n",
      "iowaresearch has 2/6417 or 0.03% with no titles\n"
     ]
    }
   ],
   "source": [
    "for source in docs_with_no_title:\n",
    "    print(\n",
    "        '{} has {}/{} or {}% with no titles'.format(\n",
    "            source['key'],\n",
    "            source['doc_count'],\n",
    "            source['bg_count'],\n",
    "            format(source['score']*100, '.2f')\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SHAREPA for SHARE Parsing and Analysis\n",
    "\n",
    "While you can always pass raw elasticsearch queries to the SHARE API, there is also a pip-installable python library that you can use that makes elasticsearch aggregations a little simpler. This library is called [sharepa - short for SHARE Parsing and Analysis](https://github.com/CenterForOpenScience/sharepa#sharepa)\n",
    "\n",
    "### Basic Actions\n",
    "\n",
    "A basic search will provide access to all documents in SHARE in 10 document slices.\n",
    "\n",
    "#### Count\n",
    "You can use sharepa and the basic search to get the total number of documents in SHARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4264118"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_search.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterating Through Results\n",
    "Executing the basic search will send the actual basic query to the SHARE API and then let you iterate through results, 10 at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avian community structure and incidence of human West Nile infection\n",
      "Rat12_a\n",
      "Non compact continuum limit of two coupled Potts models\n",
      "\n",
      "Simultaneous Localization, Mapping, and Manipulation for Unsupervised\n",
      "  Object Discovery\n",
      "Synthesis of High-Temperature Self-lubricating Wear Resistant Composite Coating on Ti6Al4V Alloy by Laser Deposition\n",
      "Comparative Studies of Silicon Dissolution in Molten Aluminum Under Different Flow Conditions, Part I: Single-Phase Flow\n",
      "Scrambling of data in all-optical domain\n",
      "Step behaviour and autonomic nervous system activity in multiparous dairy cows during milking in a herringbone milking system\n",
      "<p>Typical features of the constant velocity forced dissociation process in the SGP-3-ligated 1G1Q 2CR complex system.</p>\n"
     ]
    }
   ],
   "source": [
    "results = basic_search.execute()\n",
    "\n",
    "for hit in results:\n",
    "    print(hit.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't want 10 results, or we want to offset the results, we can use slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements of Trust in Named-Data Networking\n",
      "Effect of Perceived Attributions about Ostracism on Social Pain and Task Performance\n",
      "Millimeter Wave MIMO Channel Tracking Systems\n",
      "Metric Dimension and Zero Forcing Number of Two Families of Line Graphs\n",
      "The Glassey conjecture on asymptotically flat manifolds\n"
     ]
    }
   ],
   "source": [
    "results = basic_search[20:25].execute()\n",
    "for hit in results:\n",
    "    print(hit.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Search with sharepa\n",
    "\n",
    "You can make your own search object, which allows you to pass in custom queries for certain terms or SHARE fields. Queries are formed using [lucene query syntax](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html#query-string-syntax), just like we used in the above examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_search = ShareSearch()\n",
    "\n",
    "my_search = my_search.query(\n",
    "    'query_string', # Type of query, will accept a lucene query string\n",
    "    query='NOT tags:*', # This lucene query string will find all documents that don't have tags\n",
    "    analyze_wildcard=True  # This will make elasticsearch pay attention to the asterisk (which matches anything)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of query accepts a 'query_string'. Other options include a match query, a multi-match query, a bool query, and any other query structure available in the elasticsearch API.\n",
    "\n",
    "We can see what that query that we're about to send to elasticsearch by using the pretty print helper function. You'll see that it looks very similar to the queries we defined by hand earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"query\": {\n",
      "        \"query_string\": {\n",
      "            \"analyze_wildcard\": true, \n",
      "            \"query\": \"NOT tags:*\"\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "pretty_print(my_search.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you execute that query, you can then iterate through the results the same way that you could with the simple search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avian community structure and incidence of human West Nile infection\n",
      "Non compact continuum limit of two coupled Potts models\n",
      "\n",
      "Simultaneous Localization, Mapping, and Manipulation for Unsupervised\n",
      "  Object Discovery\n",
      "Synthesis of High-Temperature Self-lubricating Wear Resistant Composite Coating on Ti6Al4V Alloy by Laser Deposition\n",
      "Comparative Studies of Silicon Dissolution in Molten Aluminum Under Different Flow Conditions, Part I: Single-Phase Flow\n",
      "Scrambling of data in all-optical domain\n",
      "Step behaviour and autonomic nervous system activity in multiparous dairy cows during milking in a herringbone milking system\n",
      "<p>Typical features of the constant velocity forced dissociation process in the SGP-3-ligated 1G1Q 2CR complex system.</p>\n",
      "The elusive shepherdess\n"
     ]
    }
   ],
   "source": [
    "new_results = my_search.execute()\n",
    "for hit in new_results:\n",
    "    print(hit.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregations with sharepa\n",
    "\n",
    "You can also use sharepa to do aggregations, like we did with the above long query.\n",
    "\n",
    "We can add an aggregation to my_search that will give us the number of documents per source that meet that previously defined search query (in our case, items that don't have tags). Here's what adding that aggregation will look like -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificantTerms(field='_type', min_doc_count=1, percentage={}, size=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_search.aggs.bucket(\n",
    "    'sources',  # Every aggregation needs a name\n",
    "    'significant_terms',  # There are many kinds of aggregations\n",
    "    field='_type',  # We store the source of a document in its type, so this will aggregate by source\n",
    "    min_doc_count=1,\n",
    "    percentage={},\n",
    "    size=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see which query is actually going to be sent to elasticsearch by printing out the query. This is very similar to the queries we were defining by hand up above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"query\": {\n",
      "        \"query_string\": {\n",
      "            \"analyze_wildcard\": true, \n",
      "            \"query\": \"NOT tags:*\"\n",
      "        }\n",
      "    }, \n",
      "    \"aggs\": {\n",
      "        \"sources\": {\n",
      "            \"significant_terms\": {\n",
      "                \"field\": \"_type\", \n",
      "                \"percentage\": {}, \n",
      "                \"min_doc_count\": 1, \n",
      "                \"size\": 0\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "pretty_print(my_search.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nist -- 100.00% do not have tags\n",
      "purdue -- 100.00% do not have tags\n",
      "sldr -- 100.00% do not have tags\n",
      "pcurio -- 99.53% do not have tags\n",
      "oaktrust -- 98.77% do not have tags\n",
      "scholarsbank -- 98.33% do not have tags\n",
      "hacettepe -- 97.78% do not have tags\n",
      "addis_ababa -- 97.68% do not have tags\n",
      "figshare -- 97.68% do not have tags\n",
      "texasstate -- 96.85% do not have tags\n",
      "calhoun -- 96.61% do not have tags\n",
      "citeseerx -- 96.19% do not have tags\n",
      "springer -- 95.89% do not have tags\n",
      "dailyssrn -- 95.63% do not have tags\n",
      "valposcholar -- 95.53% do not have tags\n",
      "columbia -- 95.36% do not have tags\n",
      "digitalhoward -- 95.02% do not have tags\n",
      "plos -- 94.98% do not have tags\n",
      "biomedcentral -- 94.76% do not have tags\n",
      "scholarsarchiveosu -- 94.44% do not have tags\n",
      "crossref -- 94.44% do not have tags\n",
      "dash -- 94.38% do not have tags\n",
      "upennsylvania -- 93.75% do not have tags\n",
      "cuscholar -- 93.60% do not have tags\n",
      "cyberleninka -- 93.48% do not have tags\n",
      "bhl -- 93.12% do not have tags\n",
      "pcom -- 93.12% do not have tags\n",
      "asu -- 92.92% do not have tags\n",
      "mason -- 92.31% do not have tags\n",
      "pubmedcentral -- 91.81% do not have tags\n",
      "uiucideals -- 91.65% do not have tags\n",
      "cmu -- 91.47% do not have tags\n",
      "erudit -- 90.93% do not have tags\n",
      "harvarddataverse -- 90.69% do not have tags\n",
      "trinity -- 90.67% do not have tags\n",
      "noaa_nodc -- 89.14% do not have tags\n",
      "ghent -- 89.01% do not have tags\n",
      "tdar -- 88.97% do not have tags\n",
      "datacite -- 88.33% do not have tags\n",
      "mblwhoilibrary -- 87.85% do not have tags\n",
      "iastate -- 87.62% do not have tags\n",
      "zenodo -- 87.09% do not have tags\n",
      "iowaresearch -- 86.68% do not have tags\n",
      "uky -- 86.45% do not have tags\n",
      "calpoly -- 86.24% do not have tags\n",
      "csuohio -- 86.22% do not have tags\n",
      "pdxscholar -- 85.64% do not have tags\n",
      "utktrace -- 85.15% do not have tags\n",
      "scholarscompass_vcu -- 84.98% do not have tags\n",
      "vtech -- 84.43% do not have tags\n",
      "caltech -- 84.13% do not have tags\n",
      "uow -- 83.76% do not have tags\n",
      "ncar -- 83.33% do not have tags\n",
      "waynestate -- 83.28% do not have tags\n",
      "duke -- 82.38% do not have tags\n",
      "stcloud -- 81.25% do not have tags\n",
      "arxiv_oai -- 81.14% do not have tags\n",
      "osf -- 80.65% do not have tags\n",
      "opensiuc -- 80.17% do not have tags\n",
      "umich -- 79.47% do not have tags\n",
      "unl_digitalcommons -- 78.42% do not have tags\n",
      "scholarworks_umass -- 78.39% do not have tags\n",
      "chapman -- 77.28% do not have tags\n",
      "spdataverse -- 76.63% do not have tags\n",
      "huskiecommons -- 76.14% do not have tags\n",
      "cambridge -- 75.70% do not have tags\n",
      "dataone -- 74.71% do not have tags\n",
      "udel -- 70.26% do not have tags\n",
      "utaustin -- 68.56% do not have tags\n",
      "smithsonian -- 67.79% do not have tags\n",
      "mit -- 66.63% do not have tags\n",
      "cogprints -- 63.16% do not have tags\n",
      "wustlopenscholarship -- 62.71% do not have tags\n",
      "rcaap -- 62.60% do not have tags\n",
      "shareok -- 60.68% do not have tags\n",
      "kent -- 60.62% do not have tags\n",
      "ucescholarship -- 57.54% do not have tags\n",
      "uwashington -- 56.26% do not have tags\n",
      "wash_state_u -- 51.40% do not have tags\n",
      "dryad -- 50.15% do not have tags\n",
      "doepages -- 48.49% do not have tags\n",
      "scitech -- 47.03% do not have tags\n",
      "nih -- 38.86% do not have tags\n",
      "icpsr -- 33.15% do not have tags\n",
      "lshtm -- 29.51% do not have tags\n",
      "clinicaltrials -- 22.85% do not have tags\n",
      "neurovault -- 13.20% do not have tags\n",
      "lwbin -- 1.99% do not have tags\n"
     ]
    }
   ],
   "source": [
    "aggregated_results = my_search.execute()\n",
    "\n",
    "for source in aggregated_results.aggregations['sources']['buckets']:\n",
    "    print(\n",
    "        '{} -- {}% do not have tags'.format(\n",
    "            source['key'], \n",
    "            format(source['score']*100, '.2f')\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
