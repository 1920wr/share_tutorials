{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex Queries and Basic Visualization\n",
    "\n",
    "This notebook will cover how to make more complex queries, using both basic HTTP requests and using sharepa - the SHARE parsing and analysis library.\n",
    "\n",
    "We'll also go over aggregations, or queries that will return summary statistics about the whole dataset. We'll use those aggregations to make some simple data visualizations using pandas and matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Here we'll define a helper function and specify the SHARE API url that we'll use for querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "OSF_APP_URL = 'https://osf.io/api/v1/share/search/'\n",
    "\n",
    "def query_share(url, query):\n",
    "    # A helper function that will use the requests library,\n",
    "    # pass along the correct headers,\n",
    "    # and make the query we want\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = json.dumps(query)\n",
    "    return requests.post(url, headers=headers, data=data, verify=False).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Queries\n",
    "\n",
    "### Pagination\n",
    "\n",
    "One request to the SHARE API will return just 10 results by default. To get more results, you can use the \"size\" parameter in your request, or paginate through the results you get back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 976 total results and 10 results on this page\n",
      "---------------\n",
      "1. Herpetology Collection - Royal Ontario Museum\n",
      "2. Herpetology Collection - Royal Ontario Museum\n",
      "3. On a collection of reptiles and frogs chiefly from Singapore.\n",
      "4. Effects of nonlethal predation and competition on life-history characteristics and behavior of larval Oregon spotted frog (Rana pretiosa) and larval red-legged frog (Rana aurora)\n",
      "5. Differences in foraging habits during the day and night time in the green frog, Rana clamitans.\n",
      "6. Evaluating group housing strategies for the ex-situ conservation of harlequin frogs (Atelopus spp.) using behavioral and physiological indicators\n",
      "7. <p>Total annual biomass (g) of recaptured breeding wood frogs at 11 vernal pools in east-central Maine, USA.</p>\n",
      "8. Correction: A Novel Reproductive Mode in Frogs: A New Species of Fanged Frog with Internal Fertilization and Birth of Tadpoles\n",
      "9. Physiologische studien über die Hemmungsmechanismen für die reflexthätigkeit des Rückenmarks im gehirne des Frosches / \n",
      "10. Uninfected control frogs and frogs infected for 6 days with FV3 were treated with BrdU 48 hrs before the assay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erin/.virtualenvs/tuts/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py:791: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "basic_query = {\n",
    "    \"query\": {\n",
    "        \"query_string\": {\n",
    "            \"query\": \"frogs\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "query_results = query_share(OSF_APP_URL, basic_query)\n",
    "\n",
    "print(\n",
    "    'There are {} total results and {} results on this page'.format(\n",
    "        query_results['count'],\n",
    "        len(query_results['results'])\n",
    "    )\n",
    ")\n",
    "\n",
    "print('---------------')\n",
    "for result in enumerate(query_results['results']):\n",
    "    print('{}. {}'.format(result[0] + 1, result[1]['title'].encode('utf-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get more results either by changing the number of results returned, or by paginating through the results.\n",
    "\n",
    "First, we'll return 20 results by specifying the size in our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erin/.virtualenvs/tuts/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py:791: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 976 total results and 20 results on this page\n",
      "---------------\n",
      "1. Herpetology Collection - Royal Ontario Museum\n",
      "2. Herpetology Collection - Royal Ontario Museum\n",
      "3. On a collection of reptiles and frogs chiefly from Singapore.\n",
      "4. Effects of nonlethal predation and competition on life-history characteristics and behavior of larval Oregon spotted frog (Rana pretiosa) and larval red-legged frog (Rana aurora)\n",
      "5. Differences in foraging habits during the day and night time in the green frog, Rana clamitans.\n",
      "6. Evaluating group housing strategies for the ex-situ conservation of harlequin frogs (Atelopus spp.) using behavioral and physiological indicators\n",
      "7. <p>Total annual biomass (g) of recaptured breeding wood frogs at 11 vernal pools in east-central Maine, USA.</p>\n",
      "8. Correction: A Novel Reproductive Mode in Frogs: A New Species of Fanged Frog with Internal Fertilization and Birth of Tadpoles\n",
      "9. Physiologische studien über die Hemmungsmechanismen für die reflexthätigkeit des Rückenmarks im gehirne des Frosches / \n",
      "10. Uninfected control frogs and frogs infected for 6 days with FV3 were treated with BrdU 48 hrs before the assay\n",
      "11. Fatigue and work capacity of muscles from frogs treated with male sex hormone\n",
      "12. Representative two-color flow cytometry analysis of splenocytes from uninfected control frogs (A), and frogs infected for 6 days (B)\n",
      "13. The frog book; North American toads and frogs, with a study of the habits and life histories of those of the northeastern states. \n",
      "14. Overwintering of Columbia spotted frogs in northeastern Oregon\n",
      "15. Mycobacterium liflandii Outbreak in a Research Colony of Xenopus (Silurana) tropicalis Frogs\n",
      "16. <i>Frankixalus</i>, a New Rhacophorid Genus of Tree Hole Breeding Frogs with Oophagous Tadpoles\n",
      "17. Residues of Polybrominated Diphenyl Ethers in Frogs (<i>Rana limnocharis</i>) from a Contaminated Site, South China: Tissue Distribution, Biomagnification, and Maternal Transfer\n",
      "18. Cryoprotectants and Extreme Freeze Tolerance in a Subarctic Population of the Wood Frog\n",
      "19. Assessment of radiocesium contamination in frogs 18 months after the Fukushima Daiichi nuclear disaster\n",
      "20. \n"
     ]
    }
   ],
   "source": [
    "basic_query = {\n",
    "    \"query\": {\n",
    "        \"query_string\": {\n",
    "            \"query\": \"frogs\"\n",
    "        }\n",
    "    },\n",
    "    \"size\": 20\n",
    "}\n",
    "\n",
    "query_results = query_share(OSF_APP_URL, basic_query)\n",
    "\n",
    "print(\n",
    "    'There are {} total results and {} results on this page'.format(\n",
    "        query_results['count'],\n",
    "        len(query_results['results'])\n",
    "    )\n",
    ")\n",
    "print('---------------')\n",
    "for result in enumerate(query_results['results']):\n",
    "    print('{}. {}'.format(result[0] + 1, result[1]['title'].encode('utf-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also paginate through results by specifying the place to start in all of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erin/.virtualenvs/tuts/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py:791: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 976 total results and 10 results on this page\n",
      "---------------\n",
      "1. Herpetology Collection - Royal Ontario Museum\n",
      "2. Herpetology Collection - Royal Ontario Museum\n",
      "3. On a collection of reptiles and frogs chiefly from Singapore.\n",
      "4. Effects of nonlethal predation and competition on life-history characteristics and behavior of larval Oregon spotted frog (Rana pretiosa) and larval red-legged frog (Rana aurora)\n",
      "5. Differences in foraging habits during the day and night time in the green frog, Rana clamitans.\n",
      "6. Evaluating group housing strategies for the ex-situ conservation of harlequin frogs (Atelopus spp.) using behavioral and physiological indicators\n",
      "7. <p>Total annual biomass (g) of recaptured breeding wood frogs at 11 vernal pools in east-central Maine, USA.</p>\n",
      "8. Correction: A Novel Reproductive Mode in Frogs: A New Species of Fanged Frog with Internal Fertilization and Birth of Tadpoles\n",
      "9. Physiologische studien über die Hemmungsmechanismen für die reflexthätigkeit des Rückenmarks im gehirne des Frosches / \n",
      "10. Uninfected control frogs and frogs infected for 6 days with FV3 were treated with BrdU 48 hrs before the assay\n",
      "---------------\n",
      "*** Making another query ***\n",
      "---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erin/.virtualenvs/tuts/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py:791: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 976 total results and 10 results on this page\n",
      "---------------\n",
      "1. Fatigue and work capacity of muscles from frogs treated with male sex hormone\n",
      "2. Representative two-color flow cytometry analysis of splenocytes from uninfected control frogs (A), and frogs infected for 6 days (B)\n",
      "3. The frog book; North American toads and frogs, with a study of the habits and life histories of those of the northeastern states. \n",
      "4. Overwintering of Columbia spotted frogs in northeastern Oregon\n",
      "5. Mycobacterium liflandii Outbreak in a Research Colony of Xenopus (Silurana) tropicalis Frogs\n",
      "6. <i>Frankixalus</i>, a New Rhacophorid Genus of Tree Hole Breeding Frogs with Oophagous Tadpoles\n",
      "7. Residues of Polybrominated Diphenyl Ethers in Frogs (<i>Rana limnocharis</i>) from a Contaminated Site, South China: Tissue Distribution, Biomagnification, and Maternal Transfer\n",
      "8. Cryoprotectants and Extreme Freeze Tolerance in a Subarctic Population of the Wood Frog\n",
      "9. Assessment of radiocesium contamination in frogs 18 months after the Fukushima Daiichi nuclear disaster\n",
      "10. \n"
     ]
    }
   ],
   "source": [
    "basic_query = {\n",
    "    \"query\": {\n",
    "        \"query_string\": {\n",
    "            \"query\": \"frogs\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "query_results = query_share(OSF_APP_URL, basic_query)\n",
    "\n",
    "print(\n",
    "    'There are {} total results and {} results on this page'.format(\n",
    "        query_results['count'],\n",
    "        len(query_results['results'])\n",
    "    )\n",
    ")\n",
    "print('---------------')\n",
    "for result in enumerate(query_results['results']):\n",
    "    print('{}. {}'.format(result[0] + 1, result[1]['title'].encode('utf-8')))\n",
    "\n",
    "print('---------------')\n",
    "print('*** Making another query ***')\n",
    "print('---------------')\n",
    "\n",
    "basic_query['from'] = 10  # Add the 'from' parameter to the query to pick up at the next page of results\n",
    "\n",
    "query_results = query_share(OSF_APP_URL, basic_query)\n",
    "\n",
    "print(\n",
    "    'There are {} total results and {} results on this page'.format(\n",
    "        query_results['count'],\n",
    "        len(query_results['results'])\n",
    "    )\n",
    ")\n",
    "print('---------------')\n",
    "for result in enumerate(query_results['results']):\n",
    "    print('{}. {}'.format(result[0] + 1, result[1]['title'].encode('utf-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pagination with sharepa\n",
    "\n",
    "You can also use sharepa to paginate through all of the results in your query, and to access slices of your query at any time. The ShareSearch object returns a generator that you can use to access all results, using slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "C extension: hashtable not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext --inplace' to build the C extensions first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-497fcd3b24df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msharepa\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mShareSearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msharepa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhelpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpretty_print\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfrogs_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mShareSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/erin/.virtualenvs/tuts/lib/python2.7/site-packages/sharepa/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msharepa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mShareSearch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasic_search\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msharepa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalysis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbucket_to_dataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_dataframes\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/erin/.virtualenvs/tuts/lib/python2.7/site-packages/sharepa/analysis.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbucket_to_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     '''A function that turns elasticsearch aggregation buckets into dataframes\n",
      "\u001b[0;32m/Users/erin/.virtualenvs/tuts/lib/python2.7/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m                       \u001b[0;34m\"pandas from the source directory, you may need to run \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                       \u001b[0;34m\"'python setup.py build_ext --inplace' to build the C \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                       \"extensions first.\".format(module))\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: C extension: hashtable not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext --inplace' to build the C extensions first."
     ]
    }
   ],
   "source": [
    "from sharepa import ShareSearch\n",
    "from sharepa.helpers import pretty_print\n",
    "\n",
    "frogs_search = ShareSearch()\n",
    "\n",
    "frogs_search = frogs_search.query(\n",
    "    'query_string',\n",
    "    query='frogs'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frog_results = frogs_search.execute()\n",
    "for result in enumerate(frog_results):\n",
    "    print('{}. {}'.format(result[0] + 1, result[1].title.encode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frog_results = frogs_search[10:20].execute()\n",
    "for result in enumerate(frog_results):\n",
    "    print('{}. {}'.format(result[0] + 1, result[1].title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregations\n",
    "\n",
    "While searching for individual results is useful, sharepa also lets you make aggregation queries that give you results across the entirety of the SHARE dataset at once. This is useful if you're curious about the completeness of data sets.\n",
    "\n",
    "For example, we can find the number of documents per source that are missing titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_titles_aggregation = {\n",
    "    \"query\": {\n",
    "        \"query_string\": {\n",
    "            \"analyze_wildcard\": True, \n",
    "            \"query\": \"NOT title:*\"\n",
    "        }\n",
    "    }, \n",
    "    \"aggs\": {\n",
    "        \"sources\": {\n",
    "            \"terms\": {\n",
    "                \"field\": \"_type\", # A field where the SHARE source is stored                \n",
    "                \"min_doc_count\": 0, \n",
    "                \"size\": 0  # Will return all sources, regardless if there are results\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_without_titles = query_share(OSF_APP_URL, missing_titles_aggregation)\n",
    "\n",
    "missing_titles_counts = results_without_titles['aggregations']['sources']['buckets']\n",
    "\n",
    "for source in missing_titles_counts:\n",
    "    print('{} has {} documents without titles'.format(source['key'], source['doc_count'], ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This information isn't terribly useful if we don't also know how many documents are in each source.\n",
    "\n",
    "Let's get that information as well, along stats for what percentage of documents from each source are missing titles. \n",
    "\n",
    "We'll do this with an elasticsearch \"sigificant terms\" aggregation. We're only interested in results that have 1 document or more, meaning all documents from the other sources have titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sig_terms_agg = {\n",
    "    \"query\": {\n",
    "        \"query_string\": {\n",
    "            \"analyze_wildcard\": True, \n",
    "            \"query\": \"NOT title:*\"\n",
    "        }\n",
    "    },\n",
    "    \"aggs\": {\n",
    "        \"sources\":{\n",
    "            \"significant_terms\":{\n",
    "                \"field\": \"_type\", # A field where the SHARE source is stored                \n",
    "                \"min_doc_count\": 1, # Only results with more than one document\n",
    "                \"percentage\": {} # This will make the \"score\" parameter a percentage\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs_with_no_title_results = query_share(OSF_APP_URL, sig_terms_agg)\n",
    "docs_with_no_title = docs_with_no_title_results['aggregations']['sources']['buckets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for source in docs_with_no_title:\n",
    "    print(\n",
    "        '{}% (or {}/{}) of documents from {} have no titles'.format(\n",
    "            format(source['score']*100, '.2f'),\n",
    "            source['doc_count'],\n",
    "            source['bg_count'],\n",
    "            source['key']\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregations with sharepa\n",
    "\n",
    "You can also use sharepa to do aggregations.\n",
    "\n",
    "Let's make a sharepa search object that will give us the number of documents per sourcethat don't have tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_tags_search = ShareSearch()\n",
    "\n",
    "no_tags_search = no_tags_search.query(\n",
    "    'query_string', # Type of query, will accept a lucene query string\n",
    "    query='NOT tags:*', # This lucene query string will find all documents that don't have tags\n",
    "    analyze_wildcard=True  # This will make elasticsearch pay attention to the asterisk (which matches anything)\n",
    ")\n",
    "\n",
    "no_tags_search.aggs.bucket(\n",
    "    'sources',  # Every aggregation needs a name\n",
    "    'significant_terms',  # There are many kinds of aggregations\n",
    "    field='_type',  # We store the source of a document in its type, so this will aggregate by source\n",
    "    min_doc_count=1,\n",
    "    percentage={},\n",
    "    size=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see which query is actually going to be sent to elasticsearch by printing out the query. This is very similar to the queries we were defining by hand up above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretty_print(no_tags_search.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aggregated_results = no_tags_search.execute()\n",
    "\n",
    "for source in aggregated_results.aggregations['sources']['buckets']:\n",
    "    print(\n",
    "        '{}% of documents from {} do not have tags'.format(\n",
    "            format(source['score']*100, '.2f'),\n",
    "            source['key'] \n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top tags \n",
    "\n",
    "Let's do an elasticsearch query to find out what the most used tags are used in the dataset across all sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_tag_search = ShareSearch()\n",
    "\n",
    "top_tag_search.aggs.bucket(\n",
    "    'tagsTermFilter',  # Every aggregation needs a name\n",
    "    'terms',  # There are many kinds of aggregations\n",
    "    field='tags',  # We store the source of a document in its type, so this will aggregate by source\n",
    "    min_doc_count=1,\n",
    "    exclude= \"of|and|or\",\n",
    "    size=10\n",
    ")\n",
    "\n",
    "# pretty_print(top_tag_search.to_dict())\n",
    "\n",
    "top_tag_results_executed = top_tag_search.execute()\n",
    "top_tag_results = top_tag_results_executed.aggregations.tagsTermFilter.to_dict()['buckets']\n",
    "\n",
    "pretty_print(top_tag_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Plotting\n",
    "\n",
    "Sharepa has some basic functions to get you started making plots using matplotlib and pandas.\n",
    "\n",
    "### Creating a dataframe from sharepa data\n",
    "\n",
    "Raw sharepa data is in the same format as elasticsearch results, represented as a nested structure. To convert the data into a format that pandas can recognize, we have to convert it into a dataframe.\n",
    "\n",
    "Let's take our top tags aggregation, make it into a pandas data frame, and plot a bar graph. Then, we'll plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "top_tags_dataframe = pd.DataFrame(top_tag_results)\n",
    "top_tags_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "top_tags_dataframe.plot(kind='bar', x='key', y='doc_count')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Queries and Dataframes\n",
    "\n",
    "Let's make a new search, for all documents updated in the years 2012 to 2015 that contain the tag \"science.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "science_search = ShareSearch() #create search object\n",
    "science_search = science_search.filter( #apply filter to search\n",
    "    \"range\", #applied a range type filter\n",
    "    providerUpdatedDateTime={ #the feild in the data we compare\n",
    "        'gte':'2012-01-01', #hits must be greater than or equal to this date and...\n",
    "        'lte':'2015-12-31' #hits must be less than or equal to this date\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "science_search = science_search.filter(\n",
    "     \"prefix\",\n",
    "     tags=\"science\"\n",
    ")\n",
    "\n",
    "science_search.aggs.bucket(\n",
    "    'sources',  # Every aggregation needs a name\n",
    "    'significant_terms',  # There are many kinds of aggregations\n",
    "    field='_type',  # We store the source of a document in its type, so this will aggregate by source\n",
    "    min_doc_count=1,\n",
    "    percentage={},\n",
    "    size=0\n",
    ")\n",
    "\n",
    "science_search_results = science_search.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out that monster of a query we just built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pretty_print(science_search.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can graph this result as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "science_results = science_search_results.aggregations.sources.to_dict()  \n",
    "science_data_frame = pd.DataFrame(science_results['buckets']) \n",
    "\n",
    "science_data_frame['percents'] = (science_data_frame['score'] * 100)\n",
    "\n",
    "science_data_frame[:30].plot(kind='bar', x='key', y='percents') # Limit to the first 30 results for readability\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the number of document that each source has. We'll limit it to the top 30 sources to make sure that the graph is readable. Here we'll use the sharepa helper function bucket_to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sharepa import bucket_to_dataframe\n",
    "\n",
    "all_results = ShareSearch()\n",
    "\n",
    "all_results = all_results.query(\n",
    "    'query_string', # Type of query, will accept a lucene query string\n",
    "    query='*', # This lucene query string will find all documents that don't have tags\n",
    "    analyze_wildcard=True  # This will make elasticsearch pay attention to the asterisk (which matches anything)\n",
    ")\n",
    "\n",
    "all_results.aggs.bucket(\n",
    "    'sources',  # Every aggregation needs a name\n",
    "    'terms',  # There are many kinds of aggregations, terms is a pretty useful one though\n",
    "    field='_type',  # We store the source of a document in its type, so this will aggregate by source\n",
    "    size=0,  # These are just to make sure we get numbers for all the sources, to make it easier to combine graphs\n",
    "    min_doc_count=0\n",
    ")\n",
    "\n",
    "all_results = all_results.execute()\n",
    "\n",
    "all_results_frame = bucket_to_dataframe('# documents by source', all_results.aggregations.sources.buckets)\n",
    "all_results_frame_sorted = all_results_frame.sort(ascending=False,  columns='# documents by source')\n",
    "all_results_frame_sorted[:30].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose different types of plots to generate. Here, we'll make a pie chart of the data sources with the top 10 most results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_results_frame_sorted[:10].plot(kind='pie', y=\"# documents by source\", legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
